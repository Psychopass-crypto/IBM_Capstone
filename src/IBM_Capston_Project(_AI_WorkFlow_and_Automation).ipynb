{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "M3shH2ReBek_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import shutil\n",
        "import time\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "register_matplotlib_converters()\n",
        "COLORS = [\"dark\",\"blue\",\"grey\"]"
      ],
      "metadata": {
        "id": "cCibSWIMCCd4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_data(data_dir):\n",
        "    \"\"\"\n",
        "    laod all json formatted files into a dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    ## input testing\n",
        "    if not os.path.isdir(data_dir):\n",
        "        raise Exception(\"specified data dir does not exist\")\n",
        "    if not len(os.listdir(data_dir)) > 0:\n",
        "        raise Exception(\"specified data dir does not contain any files\")\n",
        "\n",
        "    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search(\"\\.json\",f)]\n",
        "    correct_columns = ['country', 'customer_id', 'day', 'invoice', 'month',\n",
        "                       'price', 'stream_id', 'times_viewed', 'year']"
      ],
      "metadata": {
        "id": "_CBmvzBPCE8q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_months = {}\n",
        "invoice_file = \"/var/Invoice1.json\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_json(invoice_file)\n",
        "    all_months[os.path.split(invoice_file)[-1]] = df\n",
        "except ValueError as e:\n",
        "    print(f\"Error reading JSON file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu0Ao8pRCTK7",
        "outputId": "51486928-c012-4889-f1a8-100ac629c490"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading JSON file: Unexpected character found when decoding object value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " for f,df in all_months.items():\n",
        "        cols = set(df.columns.tolist())\n",
        "        if 'StreamID' in cols:\n",
        "             df.rename(columns={'StreamID':'stream_id'},inplace=True)\n",
        "        if 'TimesViewed' in cols:\n",
        "            df.rename(columns={'TimesViewed':'times_viewed'},inplace=True)\n",
        "        if 'total_price' in cols:\n",
        "            df.rename(columns={'total_price':'price'},inplace=True)\n",
        "\n",
        "        cols = df.columns.tolist()\n",
        "        if sorted(cols) != correct_columns:\n",
        "            raise Exception(\"columns name could not be matched to correct cols\")"
      ],
      "metadata": {
        "id": "GN1jC0ctDCoh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df = pd.concat(list(all_months.values()),sort=True)\n",
        "    years,months,days = df['year'].values,df['month'].values,df['day'].values\n",
        "    dates = [\"{}-{}-{}\".format(years[i],str(months[i]).zfill(2),str(days[i]).zfill(2)) for i in range(df.shape[0])]\n",
        "    df['invoice_date'] = np.array(dates,dtype='datetime64[D]')\n",
        "    df['invoice'] = [re.sub(\"\\D+\",\"\",i) for i in df['invoice'].values]\n",
        "\n",
        "    ## sort by date and reset the index\n",
        "    df.sort_values(by='invoice_date',inplace=True)\n",
        "    df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    return(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "-eN0m1WYCWRA",
        "outputId": "e9ce3645-a175-4543-f24c-f3970faf2407"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-53-d0cf88443cbc>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-53-d0cf88443cbc>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    years,months,days = df['year'].values,df['month'].values,df['day'].values\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_ts(df_orig, country=None):\n",
        "    \"\"\"\n",
        "    given the original DataFrame (fetch_data())\n",
        "    return a numerically indexed time-series DataFrame\n",
        "    by aggregating over each day\n",
        "    \"\"\"\n",
        "\n",
        "    if country:\n",
        "        if country not in np.unique(df_orig['country'].values):\n",
        "            raise Excpetion(\"country not found\")\n",
        "\n",
        "        mask = df_orig['country'] == country\n",
        "        df = df_orig[mask]\n",
        "    else:\n",
        "        df = df_orig\n",
        ""
      ],
      "metadata": {
        "id": "BKKzsQajFBAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  invoice_dates = df['invoice_date'].values\n",
        "    start_month = '{}-{}'.format(df['year'].values[0],str(df['month'].values[0]).zfill(2))\n",
        "    stop_month = '{}-{}'.format(df['year'].values[-1],str(df['month'].values[-1]).zfill(2))\n",
        "    df_dates = df['invoice_date'].values.astype('datetime64[D]')\n",
        "    days = np.arange(start_month,stop_month,dtype='datetime64[D]')\n",
        "\n",
        "    purchases = np.array([np.where(df_dates==day)[0].size for day in days])\n",
        "    invoices = [np.unique(df[df_dates==day]['invoice'].values).size for day in days]\n",
        "    streams = [np.unique(df[df_dates==day]['stream_id'].values).size for day in days]\n",
        "    views =  [df[df_dates==day]['times_viewed'].values.sum() for day in days]\n",
        "    revenue = [df[df_dates==day]['price'].values.sum() for day in days]\n",
        "    year_month = [\"-\".join(re.split(\"-\",str(day))[:2]) for day in days]\n",
        "\n",
        "    df_time = pd.DataFrame({'date':days,\n",
        "                            'purchases':purchases,\n",
        "                            'unique_invoices':invoices,\n",
        "                            'unique_streams':streams,\n",
        "                            'total_views':views,\n",
        "                            'year_month':year_month,\n",
        "                            'revenue':revenue})\n",
        "    return(df_time)"
      ],
      "metadata": {
        "id": "zdwP6ne3FGyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_ts(data_dir, clean=False):\n",
        "    \"\"\"\n",
        "    convenience function to read in new data\n",
        "    uses csv to load quickly\n",
        "    use clean=True when you want to re-create the files\n",
        "    \"\"\"\n",
        "\n",
        "    ts_data_dir = os.path.join(data_dir,\"ts-data\")\n",
        "\n",
        "    if clean:\n",
        "        shutil.rmtree(ts_data_dir)\n",
        "    if not os.path.exists(ts_data_dir):\n",
        "        os.mkdir(ts_data_dir)\n",
        "\n",
        "    ## if files have already been processed load them\n",
        "    if len(os.listdir(ts_data_dir)) > 0:\n",
        "        print(\"... loading ts data from files\")\n",
        "        return({re.sub(\"\\.csv\",\"\",cf)[3:]:pd.read_csv(os.path.join(ts_data_dir,cf)) for cf in os.listdir(ts_data_dir)})\n"
      ],
      "metadata": {
        "id": "O0NAQLzmFKcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"... processing data for loading\")\n",
        "    df = fetch_data(data_dir)\n",
        "\n",
        "    ## find the top ten countries (wrt revenue)\n",
        "    table = pd.pivot_table(df,index='country',values=\"price\",aggfunc='sum')\n",
        "    table.columns = ['total_revenue']\n",
        "    table.sort_values(by='total_revenue',inplace=True,ascending=False)\n",
        "    top_ten_countries =  np.array(list(table.index))[:10]\n",
        "\n",
        "    file_list = [os.path.join(data_dir,f) for f in os.listdir(data_dir) if re.search(\"\\.json\",f)]\n",
        "    countries = [os.path.join(data_dir,\"ts-\"+re.sub(\"\\s+\",\"_\",c.lower()) + \".csv\") for c in top_ten_countries]\n",
        "\n",
        "    ## load the data\n",
        "    dfs = {}\n",
        "    dfs['all'] = convert_to_ts(df)\n",
        "    for country in top_ten_countries:\n",
        "        country_id = re.sub(\"\\s+\",\"_\",country.lower())\n",
        "        file_name = os.path.join(data_dir,\"ts-\"+ country_id + \".csv\")\n",
        "        dfs[country_id] = convert_to_ts(df,country=country)\n",
        "\n",
        "    ## save the data as csvs\n",
        "    for key, item in dfs.items():\n",
        "        item.to_csv(os.path.join(ts_data_dir,\"ts-\"+key+\".csv\"),index=False)\n",
        "\n",
        "    return(dfs)\n"
      ],
      "metadata": {
        "id": "pGRGHhlcFOau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df,training=True):\n",
        "    \"\"\"\n",
        "    for any given day the target becomes the sum of the next days revenue\n",
        "    for that day we engineer several features that help predict the summed revenue\n",
        "\n",
        "    the 'training' flag will trim data that should not be used for training\n",
        "    when set to false all data will be returned\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ## extract dates\n",
        "    dates = df['date'].values.copy()\n",
        "    dates = dates.astype('datetime64[D]')\n",
        "\n",
        "    ## engineer some features\n",
        "    eng_features = defaultdict(list)\n",
        "    previous =[7, 14, 28, 70]  #[7, 14, 21, 28, 35, 42, 49, 56, 63, 70]\n",
        "    y = np.zeros(dates.size)\n",
        "    for d,day in enumerate(dates):\n",
        "\n"
      ],
      "metadata": {
        "id": "5_XNIbsXFR_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  ## use windows in time back from a specific date\n",
        "        for num in previous:\n",
        "            current = np.datetime64(day, 'D')\n",
        "            prev = current - np.timedelta64(num, 'D')\n",
        "            mask = np.in1d(dates, np.arange(prev,current,dtype='datetime64[D]'))\n",
        "            eng_features[\"previous_{}\".format(num)].append(df[mask]['revenue'].sum())\n",
        "\n",
        "        ## get get the target revenue\n",
        "        plus_30 = current + np.timedelta64(30,'D')\n",
        "        mask = np.in1d(dates, np.arange(current,plus_30,dtype='datetime64[D]'))\n",
        "        y[d] = df[mask]['revenue'].sum()\n",
        "\n",
        "        ## attempt to capture monthly trend with previous years data (if present)\n",
        "        start_date = current - np.timedelta64(365,'D')\n",
        "        stop_date = plus_30 - np.timedelta64(365,'D')\n",
        "        mask = np.in1d(dates, np.arange(start_date,stop_date,dtype='datetime64[D]'))\n",
        "        eng_features['previous_year'].append(df[mask]['revenue'].sum())\n",
        "\n",
        "        ## add some non-revenue features\n",
        "        minus_30 = current - np.timedelta64(30,'D')\n",
        "        mask = np.in1d(dates, np.arange(minus_30,current,dtype='datetime64[D]'))\n",
        "        eng_features['recent_invoices'].append(df[mask]['unique_invoices'].mean())\n",
        "        eng_features['recent_views'].append(df[mask]['total_views'].mean())\n",
        "\n",
        "    X = pd.DataFrame(eng_features)"
      ],
      "metadata": {
        "id": "CvvlnLLIFvxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ## combine features in to df and remove rows with all zeros\n",
        "    X.fillna(0,inplace=True)\n",
        "    mask = X.sum(axis=1)>0\n",
        "    X = X[mask]\n",
        "    y = y[mask]\n",
        "    dates = dates[mask]\n",
        "    X.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if training == True:\n",
        "        ## remove the last 30 days (because the target is not reliable)\n",
        "        mask = np.arange(X.shape[0]) < np.arange(X.shape[0])[-30]\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "        dates = dates[mask]\n",
        "        X.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return(X,y,dates)\n",
        "\n"
      ],
      "metadata": {
        "id": "r4gr88J1F0Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    run_start = time.time()\n",
        "    data_dir = os.path.join(\"..\",\"data\",\"cs-train\")\n",
        "    print(\"...fetching data\")\n",
        "\n",
        "    ts_all = fetch_ts(data_dir,clean=False)\n",
        "\n",
        "    m, s = divmod(time.time()-run_start,60)\n",
        "    h, m = divmod(m, 60)\n",
        "    print(\"load time:\", \"%d:%02d:%02d\"%(h, m, s))\n",
        "\n",
        "    for key,item in ts_all.items():\n",
        "        print(key,item.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "lFOiTLiPF3Hp",
        "outputId": "676256e6-df45-406c-b001-43d4f0097eb1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...fetching data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fetch_ts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-901cc8c54a20>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...fetching data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mts_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrun_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fetch_ts' is not defined"
          ]
        }
      ]
    }
  ]
}